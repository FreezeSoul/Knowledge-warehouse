# 00-集群环境准备

### 主机环境

节点配置：

| 资源   | 配置                                             |
| ---- | ---------------------------------------------- |
| 节点数  | 4                                              |
| 操作系统 | CentOS Linux release 7.4.1708 (Core)           |
| CPU  | Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz 6核12线程 |
| 内存   | 16GB                                           |

### 软件环境

**注：** CDH5的所有软件可以在此下载：[http://archive.cloudera.com/cdh5/cdh/5/](http://archive.cloudera.com/cdh5/cdh/5/)

| 软件     | 版本                     | 下载地址                                                                                                                      |
| ------ | ---------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| jdk    | jdk-8u172-linux-x64    | [点击下载](http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/jdk-8u172-linux-x64.tar.gz)|
| hadoop | hadoop-2.6.0-cdh5.14.2 | [点击下载](http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.14.2.tar.gz)|
|zookeeeper | zookeeper-3.4.5-cdh5.14.2 |[点击下载](http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2.tar.gz) |
|hbase  |hbase-1.2.0-cdh5.14.2 |[点击下载](http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.14.2.tar.gz)|
|hive|hive-1.1.0-cdh5.14.2|[点击下载](http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.14.2.tar.gz)|
|kylin|apache-kylin-2.4.0-bin-cdh57|[点击下载](http://mirrors.hust.edu.cn/apache/kylin/apache-kylin-2.4.0/apache-kylin-2.4.0-bin-cdh57.tar.gz)|
|spark on yarn|spark-2.3.1-bin-hadoop2.6|[点击下载](https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.6.tgz)|
|kafka|kafka_2.12-1.1.0|[点击下载](http://mirrors.hust.edu.cn/apache/kafka/1.1.0/kafka_2.12-1.1.0.tgz)|
|kafka-manager|kafka-manager-1.3.3.17|[点击下载](https://github.com/yahoo/kafka-manager/archive/1.3.3.17.tar.gz)|
|azkaban|azkaban-3.59.0|[git地址](https://github.com/azkaban/azkaban.git)  |


kafka-manager官方只提供源码下载，下载后编译：sbt clean dist，此编译需要x翻XX墙x才能完成，没有条件的话可以下载网友编译好的低一点的版本：[kafka-manager-1.3.3.4](https://pan.baidu.com/s/1miDMuyG)

### 主机规划

5个节点角色规划如下：

|主机名   |pycdhnode1		 |pycdhnode2		 |pycdhnode3		 |pycdhnode4		 |
| ------ | ---------------------- | ----------------- | ---------------------------- | ----------------------------------------- |
|IP	|192.168.0.158		|192.168.0.159		|192.168.0.160		|192.168.0.161		|
|namenode|yes	|yes	|no	|no	|
|dataNode|yes	|yes	|yes	|yes	|
|resourcemanager|ye	|yes	|no	|no	|
|nodemanager|yes	|yes	|yes	|yes	|
|journalnode|no		|yes	|yes	|yes	|
|JobHistoryServer|yes	|yes	|yes	yes|
|zookeeper|no		|yes	|yes	|yes	|
|hmaster(hbase)|yes	|yes	|no	|no	|
|regionserver(hbase)|no	|no	|yes	|yes	|
|hive(hiveserver2)|yes	|yes	|no	|no	|
|mysql|no	|no	|no	|yes	|
|kafka|no	|yes	|yes	|yes	|
|kafka-manager|no	|no	|yes	|no	|
|kylin|yes	|no	|no	|no	|
|spark on yarn|yes	|yes	|yes	|yes	|
|azkaban-ExecutorServer|no	|yes	|yes	|yes	|
|azkaban-WebServer|yes	|no	|no	|no	|

**注：** Journalnode和ZooKeeper保持奇数个，如果需要高可用则不少于 3 个节点。具体原因，以后详叙。

### 主机安装前准备

1. 关闭所有节点的 `SELinux`

```
sed -i 's/^SELINUX=.*$/SELINUX=disabled/g' /etc/selinux/config
setenforce 0
```

1. 关闭所有节点防火墙 `firewalld` or `iptables`  

```
systemctl disable firewalld;
systemctl stop firewalld;
systemctl disable iptables;
systemctl stop iptables;
```

1. 开启所有节点时间同步 `ntpdate`

```
echo "*/5 * * * * /usr/sbin/ntpdate asia.pool.ntp.org | logger -t NTP" >> /var/spool/cron/root
```

1. 设置所有节点语言编码以及时区

```
echo 'export TZ=Asia/Shanghai' >> /etc/profile
echo 'export LANG=en_US.UTF-8' >> /etc/profile
. /etc/profile
```

1. 所有节点添加hadoop用户

```
useradd -d /application/hadoop hadoop
echo 'hadoop' | passwd --stdin hadoop
# 设置PS1
su - hadoop
echo 'export PS1="\u@\h:\$PWD>"' >> ~/.bash_profile
echo "alias mv='mv -i'
alias rm='rm -i'" >> ~/.bash_profile
. ~/.bash_profile
```

1. 所有主机添加hosts文件 `vi /etc/hosts`,添加以下内容：

```
192.168.0.158 pypycdhnode1
192.168.0.159 pypycdhnode2
192.168.0.160 pypycdhnode3
192.168.0.161 pypycdhnode4
```

1. 设置hadoop用户之间免密登录   
   首先在pypycdhnode1主机生成秘钥

```
su - hadoop
ssh-keygen -t rsa # 一直回车即可生成hadoop用户的公钥和私钥
cd .ssh
vi id_rsa.pub # 去掉公钥末尾的 账号@主机名 : hadoop@pypycdhnode1
cat id_rsa.pub > authorized_keys
chmod 600 authorized_keys
```

压缩.ssh文件夹

```
su - hadoop
zip -r ssh.zip .ssh
```

随后分发ssh.zip到pypycdhnode2-4主机hadoop用户家目录解压即完成免密登录

1. 主机内核参数优化以及最大文件打开数、最大进程数等参数优化   
   不同主机优化参数有可能不一样，故这里不作出具体优化方法，但如果Hadoop环境用于正式生产，必须优化，linux默认参数可能会导致hadoop集群性能低下。   

2. datanode节点（pypycdhnode1-4）创建hdfs数据目录/chunk1，授权给hadoop用户

```
mkdir /application/chunk1
ln -sf /application/chunk1 /
chown -R hadoop. /application/chunk1
chown -R hadoop. /chunk1
```

**注：** 以上操作需要使用 `root` 用户，到目前为止操作系统环境已经准备完成，以下开始正式安装，后面的操作如果不做特殊说明均使用 `hadoop` 用户

### 安装jdk1.8

所有节点都需要安装，安装方式都一样   
解压 `jdk-8u172-linux-x64.tar.gz`

```
tar zxvf jdk-8u172-linux-x64.tar.gz
mkdir -p /application/hadoop/app
mv jdk-8u172-linux-x64 /application/hadoop/app/jdk
rm -f jdk-8u172-linux-x64.tar.gz
```

配置环境变量   
`vi ~/.bash_profile` 添加以下内容：

```
#java
export JAVA_HOME=/application/hadoop/app/jdk
export CLASSPATH=.:$JAVA_HOME/lib:$CLASSPATH
export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin
```

加载环境变量

```
. ~/.bash_profile
```

查看是否安装成功 `java -version`

```
java version "1.8.0_172"
Java(TM) SE Runtime Environment (build 1.8.0_172-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.172-b11, mixed mode)
```
