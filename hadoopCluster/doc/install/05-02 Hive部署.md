# 05-02 Hive部署


## mysql中创建hive数据库与用户
```
mysql > create database hivedb character set latin1 collate latin1_bin; # 这里必须设置hivedb字符集为latin1 

# mysql > grant all privileges on hivedb.* to 'hive'@'%' identified identified by 'hive'; # mysql 8.0以前可以使用，8.0报错，可以使用以下的方法 

mysql > create user 'hive'@'%' identified by 'hive'; 
mysql > grant all privileges on hivedb.* to 'hive'@'%'; 
mysql > flush privileges;
```

## pycdhnode1部署
- pycdhnode1上面解压 hive-1.1.0-cdh5.14.2.tar.gz
```
tar zxvf hive-1.1.0-cdh5.14.2.tar.gz 
mv hive-1.1.0-cdh5.14.2 /application/hadoop/app/hive 
rm -f hive-1.1.0-cdh5.14.2.tar.gz
```
- 下载mysql连接驱动并拷贝到/application/hadoop/app/hive/lib下: 
下载地址: [mysql-connector-java-8.0.11](https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-8.0.11.tar.gz)
```
tar zxvf mysql-connector-java-8.0.11.tar.gz 
cp mysql-connector-java-8.0.11/mysql-connector-java-8.0.11.jar /application/hadoop/app/hive/lib/ 
rm -f mysql-connector-java-8.0.11.tar.gz
```
这里使用的mysql为8.0.11版本，对应的mysql-connector-java也下载的8.0.11版本，如果使用的其他mysql版本，下载对应的驱动即可

- 设置环境变量 
```
vi ~/.bash_profile 添加以下内容：

#hive 
export HIVE_HOME=/application/hadoop/app/hive 
export PATH=$PATH:$HIVE_HOME/bin
```
- 加载环境变量
```
. ~/.bash_profile
```
- 进入/application/hadoop/app/hive/conf，创建hive-env.sh
```
cp hive-env.sh.template hive-env.sh
```
- 编辑 vi /application/hadoop/app/hive/conf/hive-env.sh,修改以下配置:
```
export HADOOP_HEAPSIZE=1024 
HADOOP_HOME=/application/hadoop/app/hadoop 
export HIVE_CONF_DIR=/application/hadoop/app/hive/conf 
export HIVE_AUX_JARS_PATH=/application/hadoop/app/hive/lib
```
- 修改 ${HIVE_HOME}/bin/hive
vim ${HIVE_HOME}/bin/hive
```
将下行注释
# sparkAssemblyPath=`ls ${SPARK_HOME}/lib/spark-assembly-*.jar`
添加
sparkAssemblyPath=`ls ${SPARK_HOME}/jars/*.jar`
  
 ```
 **注意：由于Spark 2.0 后jar包位置发生变化，因此需要修改 hive 的启动文件。**
 
- 添加hive配置文件 vi /application/hadoop/app/hive/conf/hive-site.xml :
```
<?xml version="1.0" encoding="UTF-8"?> 
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?> 

<configuration> 
<property> 
<name>hive.exec.scratchdir</name> 
<value>hdfs://cluster1/hive/scratchdir</value> 
<description>HDFS路径，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。</description> 
</property> 

<property> 
<name>hive.metastore.warehouse.dir</name> 
<value>hdfs://cluster1/hive/warehouse</value> 
<description>HDFS路径，用于存储hive数据文件</description> 
</property> 

<!-- 相关日志目录设置 --> 
<property> 
<name>hive.querylog.location</name> 
<value>/application/hadoop/app/hive/logs</value> 
</property> 
<property> 
<name>hive.downloaded.resources.dir</name> 
<value>/application/hadoop/data/hive/local/${hive.session.id}_resources</value> 
</property> 
<property> 
<name>hive.server2.logging.operation.log.location</name> 
<value>/application/hadoop/app/hive/logs/operation_logs</value> 
</property> 

<!-- 存储元数据的mysql连接信息 --> 
<property> 
<name>javax.jdo.option.ConnectionURL</name> 
<!--<value>jdbc:mysql://pycdhnode4:3306/hivedb?characterEncoding=UTF-8&createDatabaseIfNotExist=true</value>--> 
<value>jdbc:mysql://pycdhnode4:3306/hivedb?characterEncoding=latin1&createDatabaseIfNotExist=true</value> 
<!--description>主要编码设置,其中的&在xml中表示 & </description--> 
</property> 
<property> 
<name>javax.jdo.option.ConnectionDriverName</name> 
<value>com.mysql.jdbc.Driver</value> 
</property> 
<property> 
<name>javax.jdo.option.ConnectionUserName</name> 
<value>hive</value> 
</property> 
<property> 
<name>javax.jdo.option.ConnectionPassword</name> 
<value>hive</value> 
</property> 

<!-- 开启hive delete update 操作 --> 
<property> 
<name>hive.support.concurrency</name> 
<value>true</value> 
</property> 
<property> 
<name>hive.enforce.bucketing</name> 
<value>false</value> 
</property> 
<property> 
<name>hive.exec.dynamic.partition.mode</name> 
<value>nonstrict</value> 
</property> 
<property> 
<name>hive.txn.manager</name> 
<value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value> 
</property> 
<property> 
<name>hive.compactor.initiator.on</name> 
<value>true</value> 
</property> 
<property> 
<name>hive.compactor.worker.threads</name> 
<!--<value>1</value>--> 
<value>5</value> 
</property> 
<property> 
<name>hive.in.test</name> 
<value>true</value> 
</property> 
<property> 
<name>hive.auto.convert.join.noconditionaltask.size</name> 
<value>10000000</value> 
</property> 


<!-- hiveserver2设置--> 
<property> 
<name>hive.server2.support.dynamic.service.discovery</name> 
<value>true</value> 
</property> 
<property> 
<name>hive.server2.zookeeper.namespace</name> 
<value>hiveserver2</value> 
<description>zookeeper namespace设置</description> 
</property> 
<property> 
<name>hive.zookeeper.quorum</name> 
<value>pycdhnode2,pycdhnode3,pycdhnode4</value> 
</property> 
<property> 
<name>hive.zookeeper.client.port</name> 
<value>2181</value> 
</property> 
<property> 
<name>hive.server2.thrift.bind.host</name> 
<value>pycdhnode1</value> 
<description>hiveserver2监听地址，每个节点不一样</description> 
</property> 
<property> 
<name>hive.server2.thrift.port</name> 
<value>10000</value> 
<description>多个HiveServer2实例的端口号要一致</description> 
</property> 
</configuration>

```

**注意：hive-site.xml文件中的hive.exec.scratchdir和hive.metastore.warehouse.dir的hdfs访问地址需要和hadoop的配置文件core-site.xml中fs.defaultFS的值一致，即hdfs://cluster1**

- 创建相关目录
```
mkdir -p /application/hadoop/data/hive/local 
mkdir -p /application/hadoop/app/hive/logs
```
- 配置log4j日志输出，进入/application/hadoop/app/hive/conf，创建hive-exec-log4j.properties与hive-log4j.properties
```
cp hive-exec-log4j.properties.template hive-exec-log4j.properties 
cp hive-log4j.properties.template hive-log4j.properties
```
- 编辑 hive-exec-log4j.properties 与 hive-log4j.properties ,修改以下配置(2个配置文件都修改):
```
hive.log.dir=/application/hadoop/app/hive/logs 
log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
```
- 初始化mysql元数据
```
hadoop@pycdhnode1:/application/hadoop>schematool -initSchema -dbType mysql 
SLF4J: Class path contains multiple SLF4J bindings. 
SLF4J: Found binding in [jar:file:/application/hadoop/app/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] 
SLF4J: Found binding in [jar:file:/application/hadoop/app/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] 
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 
Metastore connection URL: jdbc:mysql://pycdhnode5:3306/hivedb?characterEncoding=latin1&createDatabaseIfNotExist=true 
Metastore Connection Driver : com.mysql.jdbc.Driver 
Metastore connection User: hive 
Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary. 
Fri Jun 29 11:37:02 CST 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 
Starting metastore schema initialization to 1.1.0-cdh5.14.2 
Initialization script hive-schema-1.1.0.mysql.sql 
Fri Jun 29 11:37:03 CST 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 
Initialization script completed 
Fri Jun 29 11:37:05 CST 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 
schemaTool completed
```
这个步骤必须做，如果未做，后面hive还是可以正常启动，但是在做数据操作时有可能就会出现卡住现象，原因是mysql元数据数据库没有初始化，导致hive在读写mysql元数据数据库是产生 metadata lock

## pycdhnode2部署
- pycdhnode2添加环境变量 .bash_profile：
```
#hive 
export HIVE_HOME=/application/hadoop/app/hive 
export PATH=$PATH:$HIVE_HOME/bin
```
- 复制hbase到pycdhnode2
``
scp -pr /application/hadoop/app/hive pycdhnode2:/application/hadoop/app 
ssh pycdhnode2 "mkdir -p /application/hadoop/data/hive/local;"
``
**注意： 传输完毕后需要修改pycdhnode2节点配置文件 hive-site.xml 中 hiverserver2的监听地址为本机**

## Hive的两种启动方式

### hive命令行模式 

用于linux平台命令行查询，查询语句基本跟mysql查询语句类似
```
$ hive
```
- 基本操作
```
hive> show databases; 
OK 
default 
Time taken: 0.08 seconds, Fetched: 1 row(s) 

hive> create database hive; 
OK 
Time taken: 0.18 seconds 

hive> show databases; 
OK 
default 
hive 

hive> use hive; 
OK 
Time taken: 0.089 seconds 

hive> create table test(id int,name string); 
OK 
Time taken: 0.331 seconds 

hive> show tables; 
OK 
test 
Time taken: 0.082 seconds, Fetched: 1 row(s) 

hive> insert into test values (1,'hello hive'); 
Query ID = hadoop_20180628105757_e64fc58b-37f6-4087-a823-738d5d933454 
Total jobs = 3 
Launching Job 1 out of 3 
Number of reduce tasks is set to 0 since there's no reduce operator 
Starting Job = job_1530153811198_0001, Tracking URL = http://pycdhnode1:8088/proxy/application_1530153811198_0001/ 
Kill Command = /application/hadoop/app/hadoop/bin/hadoop job -kill job_1530153811198_0001 
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0 
2018-06-28 10:57:38,636 Stage-1 map = 0%, reduce = 0% 
2018-06-28 10:57:53,893 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.27 sec 
MapReduce Total cumulative CPU time: 1 seconds 270 msec 
Ended Job = job_1530153811198_0001 
Stage-4 is selected by condition resolver. 
Stage-3 is filtered out by condition resolver. 
Stage-5 is filtered out by condition resolver. 
Moving data to: hdfs://cluster1/hive/warehouse/hive.db/test/.hive-staging_hive_2018-06-28_10-57-11_240_3145632387075179354-1/-ext-10000 
Loading data to table hive.test 
Table hive.test stats: [numFiles=1, numRows=1, totalSize=13, rawDataSize=12] 
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1 Cumulative CPU: 1.27 sec HDFS Read: 3706 HDFS Write: 78 SUCCESS 
Total MapReduce CPU Time Spent: 1 seconds 270 msec 
OK 
Time taken: 44.177 seconds 

hive> select * from test; 
OK 
1 hello hive 
Time taken: 0.146 seconds, Fetched: 1 row(s)
```
**注：hive默认配置不支持update和delete操作，会报错：**

- 错误处理
```
FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations.
```
解决方法，在 hive-site.xml中添加相关参数，具体配置参见 /application/hadoop/app/hive/conf/hive-site.xml：

重启启动hive，执行delete语句，还是会报错：
```
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table hive.test that does not use an AcidOutputFormat or is not bucketed
```
说是要进行delete操作的表test不是AcidOutputFormat或没有分桶。估计是要求输出是AcidOutputFormat然后必须分桶。网上查到确实如此，而且目前只有ORCFileformat支持AcidOutputFormat，不仅如此建表时必须指定参数(‘transactional’ = true)。感觉太麻烦了。。。。

照网上重新建表：
```
hive> create table test(id int ,name string )clustered by (id) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true'); 
hive> insert into table test values (1,'row1'),(2,'row2'),(3,'row3'); 
hive> delete from test where id = 1; 
hive> delete from test where name = 'row2'; 
hive> update test set name = 'Raj' where id = 3;
```
执行delete，update语句正常

### hive 远程服务 (默认端口号10000) 启动方式 

#### 启动hiveserver2 

**在生产环境中使用Hive，强烈建议使用HiveServer2来提供服务，好处很多：**

- 在应用端不用部署Hadoop和Hive客户端；

相比hive-cli方式，HiveServer2不用直接将HDFS和Metastore暴漏给用户；

- 有安全认证机制，并且支持自定义权限校验；

- 配合zookeeper有HA机制，解决应用端的并发和负载均衡问题；

- JDBC方式，可以使用任何语言，方便与应用进行数据交互；

- 从2.0开始，HiveServer2提供了WEB UI。

分别启动pycdhnode1-2主机的hiveserver2：
```
nohup bin/hiveserver2 > ${HIVE_HOME}/logs/hiveserver2.log 2>&1 & 
```
启动zk cli查看注册的hiveserver2
```
/application/hadoop/app/zookeeper/bin/zkCli.sh -server pycdhnode2:2181,pycdhnode3:2181,pycdhnode4:2181 
[zk: pycdhnode2:2181,pycdhnode3:2181,pycdhnode4:2181(CONNECTED) 1] ls /hiveserver2 
[serverUri=pycdhnode2:10000;version=1.1.0-cdh5.14.2;sequence=0000000007, serverUri=pycdhnode1:10001;version=1.1.0-cdh5.14.2;sequence=0000000008]
```

可以看到2台主机的hiveserver2均注册了

#### 使用beeline验证hiveserver2
```
hadoop@pycdhnode2:/application>beeline -u jdbc:hive2://localhost:10000 -n hadoop
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/application/hadoop/app/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/application/hadoop/app/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
scan complete in 1ms
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 1.1.0-cdh5.14.2)
Driver: Hive JDBC (version 1.1.0-cdh5.14.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.1.0-cdh5.14.2 by Apache Hive
0: jdbc:hive2://localhost:10000> show databases;
INFO  : Compiling command(queryId=hadoop_20181025140303_9031f25c-1587-4f2a-8417-278e2cb5eb42): show databases
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)
INFO  : Completed compiling command(queryId=hadoop_20181025140303_9031f25c-1587-4f2a-8417-278e2cb5eb42); Time taken: 0.021 seconds
INFO  : Executing command(queryId=hadoop_20181025140303_9031f25c-1587-4f2a-8417-278e2cb5eb42): show databases
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=hadoop_20181025140303_9031f25c-1587-4f2a-8417-278e2cb5eb42); Time taken: 0.028 seconds
INFO  : OK
+----------------+--+
| database_name  |
+----------------+--+
| default        |
| hive           |
+----------------+--+
2 rows selected (0.086 seconds)
0: jdbc:hive2://localhost:10000> create database test123;
INFO  : Compiling command(queryId=hadoop_20181025135959_6820a38c-ef48-4bbb-b507-83f7a4741ed5): create database test123
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=hadoop_20181025135959_6820a38c-ef48-4bbb-b507-83f7a4741ed5); Time taken: 0.016 seconds
INFO  : Executing command(queryId=hadoop_20181025135959_6820a38c-ef48-4bbb-b507-83f7a4741ed5): create database test123
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=hadoop_20181025135959_6820a38c-ef48-4bbb-b507-83f7a4741ed5); Time taken: 0.186 seconds
INFO  : OK
No rows affected (0.213 seconds)
0: jdbc:hive2://localhost:10000> 
```

这里还没有开启hiveserver2的账号验证，使用hadoop用户登录

```
hadoop@pycdhnode2:/application/hadoop/app/hive>hadoop fs -ls /hive/warehouse
Found 7 items
drwx-wx-wx   - hadoop supergroup          0 2018-10-25 09:53 /hive/warehouse/hive.db
drwx-wx-wx   - hadoop supergroup          0 2018-07-04 09:06 /hive/warehouse/kylin_account
drwx-wx-wx   - hadoop supergroup          0 2018-07-04 09:06 /hive/warehouse/kylin_cal_dt
drwx-wx-wx   - hadoop supergroup          0 2018-07-04 09:06 /hive/warehouse/kylin_category_groupings
drwx-wx-wx   - hadoop supergroup          0 2018-07-04 09:06 /hive/warehouse/kylin_country
drwx-wx-wx   - hadoop supergroup          0 2018-07-04 09:06 /hive/warehouse/kylin_sales
drwx-wx-wx   - hadoop supergroup          0 2018-10-25 13:59 /hive/warehouse/test123.db
```

这样使用HiveServer2时候，将非常危险，因为任何人都可以作为超级用户来操作Hive及HDFS数据。开启hive的用户安全认证后面再补充。

## 停止hiveserver2 
查找到hiveserver2相关进程id，然后kill id即可。

