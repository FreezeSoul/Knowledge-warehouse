# 03- 01理解kubernetes网络：pods

<!-- This post is going to attempt to demystify the several layers of networking operating in a[kubernetes](https://kubernetes.io/)cluster. Kubernetes is a powerful platform embodying many intelligent design choices, but discussing the way things interact can get confusing: pod networks, service networks, cluster IPs, container ports, host ports, node ports… I’ve seen a few eyes glaze over. We mostly talk about these things at work, cutting across all layers at once because something is broken and someone wants it fixed. If you take it a piece at a time and get clear on how each layer works it all makes sense in a rather elegant way. -->
这篇文章试图将 [kubernetes](https://kubernetes.io/) 集群的网络层次介绍清楚。kubernetes 是一个功能强大的平台， 有很多巧妙的设计，但是讨论其中各个部分的沟通方式常常让人感到困惑: `pod network`, `service networks`, `cluster IPs`, `container ports`, `host ports`, `node ports`…我看到了很多呆滞的眼睛。如果我们将每个层次弄清楚，一切将变得很清晰。


<!-- In order to keep things focused I’m going to split the post into three parts. This first part will look at containers and pods. The[second will examine services](https://medium.com/@betz.mark/understanding-kubernetes-networking-services-f0cb48e4cc82), which are the abstraction layer that allows pods to be ephemeral. The[last post](https://medium.com/@betz.mark/understanding-kubernetes-networking-ingress-1bc341c84078)will look at ingress and getting traffic to your pods from outside the cluster. A few disclaimers first. This post isn’t intended to be a basic intro to containers, kubernetes or pods. To learn more about how containers work see[this overview](https://docs.docker.com/engine/docker-overview/#the-underlying-technology)from[Docker](https://www.docker.com/). A high level overview of kubernetes can be f[ound here](https://kubernetes.io/), and an overview of pods specifically[is here](https://kubernetes.io/docs/concepts/workloads/pods/pod/). Lastly a basic familiarity with[networking](https://www.digitalocean.com/community/tutorials/an-introduction-to-networking-terminology-interfaces-and-protocols)and[IP address spaces](https://www.digitalocean.com/community/tutorials/understanding-ip-addresses-subnets-and-cidr-notation-for-networking)will be helpful.-->
为了将kubernetes网络说清楚，我将整片文章分成了三个部分。第一部分将介绍 `containers` 和 `pods`。第二部分将介绍 `service` ，service是一个抽象层， 用于屏蔽pods层。最后一部分将介绍 `ingress` 和从集群外部访问 pods。想了解更多关于容器的知识可查看 [this overview](https://docs.docker.com/engine/docker-overview/#the-underlying-technology)。kubernetes 更多的了解 查看 [这里](https://kubernetes.io/),pods 相关信息查看[这里](https://kubernetes.io/docs/concepts/workloads/pods/pod/)。
Lastly a basic familiarity with[networking](https://www.digitalocean.com/community/tutorials/an-introduction-to-networking-terminology-interfaces-and-protocols)and[IP address spaces](https://www.digitalocean.com/community/tutorials/understanding-ip-addresses-subnets-and-cidr-notation-for-networking)will be helpful.


![](https://cdn-images-1.medium.com/max/1600/1*5DymPHFgLmQ3WoEbbyM52Q.png)

### Pods

<!-- So what is a pod?A pod consists of one or more containers that are collocated on the same host, and are configured to share a network stack and other resources such as volumes.Pods are the basic unit kubernetes applications are built from. What does “share a network stack” actually mean? In practical terms it means that all the containers in a pod can reach each other on localhost. If I have a container running nginx and listening on port 80 and another container running scrapyd the second container can connect to the first as http://localhost:80. But how does that really work? Lets look at the typical situation when we start a docker container on our local machine: -->
所以到底什么是 pods ？一个 pods 包含了**一个或者多个**容器，这些（如果是多个）容器被分派到同一台主机上，并且分享一个网络和其他资源比如自盘空卷。pods 是 kuberbetes 的最基本的单元。分享同一个网络到底意味着什么？ 它意味着在同一个pods之中的容器可以使用 localhost 相互访问。比如有一个容器运行 nginx 并监听 80 端口，同时在同一个pod中有另一个容器运行 `scrapyd`，第二容器可以使用  http://localhost:80 连接到第一个容器。 但是这一切是如何工作的呢？让我们看一下，当我们启动一个容器时的情形。

![](https://cdn-images-1.medium.com/max/1600/1*0Xo-WpbTTGKZhJt7TvFLZQ.png)

<!-- From the top down we have a physical network interface eth0. Attached to that is a bridge docker0, and attached to that is a virtual network interface veth0. Note that docker0 and veth0 are both on the same network, 172.17.0.0/24 in this example. On this network docker0 is assigned 172.17.0.1 and is the[default gateway](https://en.wikipedia.org/wiki/Default_gateway)for veth0, which is assigned 172.17.0.2. Due to how network namespaces are configured when the container is launched processes inside it see only veth0, and communicate with the outside world through docker0 and eth0. Now let’s launch a second container: -->
从上到下，我们有一个物理网络接口eth0。eth0桥接到docker0，docker0连接到一个虚拟网络接口veth0。请注意，docker0和veth0在同一网段，本例中为172.17.0.0/24。在此网络上，docker0被分配为172.17.0.1，并且是veth0 的默认网关，veth0 被分配为172.17.0.2。由于在启动容器时会配置网络 namespace，其中的进程只能看到veth0，并通过docker0和eth0与外界通信。现在让我们启动第二个容器：

![](https://cdn-images-1.medium.com/max/1600/1*ZdgIoY6tuOqK-r6wgL7d5A.png)

<!-- As shown above the second container gets a new virtual network interface veth1, connected to the same docker0 bridge. This interface is assigned 172.17.0.3, so it is on the same logical network as the bridge and the first container, and both containers can communicate through the bridge as long as they can discover the other container’s IP address somehow. -->
上图所示的第二个容器获得了一个新的虚拟网络接口  veth1 ,连接到相同的网桥docker0。接口ip地址被分配为172.17.0.3, 所以一个和容器与第二个容器在同一个逻辑网络， 两个容器可以使用对端的IP地址，通过网桥相互通信。


<!-- That’s fine and all but it doesn’t get us to the “shared network stack” of a kubernetes pod. Fortunately namespaces are very flexible. Docker can start a container and rather than creating a new virtual network interface for it, specify that it[shares an existing interface](https://docs.docker.com/engine/reference/run/#network-settings). In this case the drawing above looks a little different: -->
这很好，但是不能解释kubernetes pod 内的共享网络。事实上 namespaces 是弹性的。Docker可以在创建一个容器时，给它[分配一个存在的网络接口](https://docs.docker.com/engine/reference/run/#network-settings)而不是重新创建一个虚拟网络。在这种情况下，与上面的图有一些不同：

![](https://cdn-images-1.medium.com/max/1600/1*akBBZKad2SAxSnJNaSHVmg.png)

<!-- Now the second container sees veth0 rather than getting its own veth1 as in the previous example. This has a few implications: first, both containers are addressable from the outside on 172.17.0.2, and on the inside each can hit ports opened by the other on localhost. This also means that the two containers cannot open the same port, which is a restriction but no different than the situation when running multiple processes on a single host. In this way a set of processes can take full advantage of the decoupling and isolation of containers, while at the same time collaborating together in the simplest possible networking environment. -->
在前面的例子中，第二个容器使用 veth0 而不是使用 veth1。含义如下：首先，两个容器可以使用外部地址172.17.0.2通信， 同时也可以使用内部 localhost 连接对方端口。这也意味着两个容器不能同时使用相同的端口，这是一个限制，但与在单个主机上运行多个进程时的情况没有什么不同。通过这种方式，一组进程可以充分利用容器的隔离机制，同时在最简单的网络环境中协作。

<!-- Kubernetes implements this pattern by creating a special container for each pod whose only purpose is to provide a network interface for the other containers. If you ssh in to a kubernetes cluster node that has pods scheduled on it and run`**docker ps**`you will see at least one container that was started with the`**pause**`command. The`**pause**`command suspends the current process until a signal is received so these containers do nothing at all except sleep until kubernetes sends them a SIGTERM. Despite this lack of activity the “pause” container is the heart of the pod, providing the virtual network interface that all the other containers will use to communicate with each other and the outside world. So in a hypothetical pod-like thing the last picture sort of looks like this: -->
Kubernetes 通过为每个 pod 创建一个特殊容器来实现此模式，其唯一目的是为其他容器提供网络接口。如果您使用 ssh 进入一个 kubernetes 集群 node ，并且该 node 上有 pod 在运行，使用 `**docker ps**` 您将看到至少一个使用 `**pause**` 命令启动的容器。 pause 命令暂停当前进程，直到收到信号，因此这些容器除睡眠外什么也不做，直到kubernetes向它们发送SIGTERM。尽管缺乏活动，"pause"容器是容器的核心，提供所有其他容器相互连接，容器与外部之间通信的虚拟网络接口。因此，最后一张图片看起来像这样：

![](https://cdn-images-1.medium.com/max/1600/1*7JLi1Rl0G0FAeu-hiTGSGQ.png)

### The Pod Network

That’s all pretty cool, but one pod full of containers that can talk to each other does not get us a system. For reasons that will become even clearer in the next post where I discuss services, the very heart of kubernetes’ design requires that pods be able to communicate with other pods whether they are running on the same local host or separate hosts. To look at how that happens we need to step up a level and look at nodes in a cluster. This section will contain some unfortunate references to network routing and routes, a subject I realize all of humanity would prefer to avoid. Finding a clear, brief tutorial on IP routing is difficult, but if you want a decent review wikipedia’s[article on the topic](https://en.wikipedia.org/wiki/Routing_table)isn’t horrible.

A kubernetes cluster consists of one or more nodes. A node is a host system, whether physical or virtual, with a container runtime and its dependencies (i.e. docker mostly) and several kubernetes system components, that is connected to a network that allows it to reach other nodes in the cluster. A simple cluster of two nodes might look like this:

![](https://cdn-images-1.medium.com/max/1600/1*XGG8e2tbP4bQbsS33gfwUw.png)

If you’re running your cluster on a cloud platform like GCP or AWS that drawing pretty well approximates the default networking architecture for a single project environment. For the purposes of illustration I’ve used the private network 10.100.0.0/24 for this example, so the router is 10.100.0.1 and the two instances are 10.100.0.2 and 10.100.0.3 respectively. Given this setup each instance can communicate with the other on eth0. That’s great, but recall that the pod we looked at above is not on this private network: it’s hanging off a bridge on a different network entirely, one that is virtual and exists only on a specific node. To make this clearer let’s drop our pod-like things back into the picture:

![](https://cdn-images-1.medium.com/max/1600/1*RiLtoAdCfcJygwePVJzZOA.png)

The host on the left has interface eth0 with an address of 10.100.0.2, whose default gateway is the router at 10.100.0.1. Connected to that interface is bridge docker0 with an address of 172.17.0.1, and connected to that is interface veth0 with address 172.17.0.2. The veth0 interface was created with the pause container and is visible inside all three containers by virtue of the shared network stack. Because of local routing rules set up when the bridge was created any packet arriving at eth0 with a destination address of 172.17.0.2 will be forwarded to the bridge, which will then send it on to veth0. Sounds ok so far. If we know we have a pod at 172.17.0.2 on this host we can add rules to our router setting the next hop for that address to 10.100.0.2 and they will get forwarded from there to veth0. Dandy! Now let’s look at the other host.

The host on the right also has eth0, with an address of 10.100.0.3, using the same default gateway at 10.100.0.1, and again connected to it is the docker0 bridge with an address of 172.17.0.1. Hmm. That’s going to be an issue. Now this address might not actually be the same as the other bridge on host 1. I’ve made it the same here because that’s a pathological worst case, and it might very well work out that way if you just installed docker and let it do its thing. But even if the chosen network is different this highlights the more fundamental problem which is that one node typically has no idea what private address space was assigned to a bridge on another node, and we need to know that if we’re going to send packets to it and have them arrive at the right place. Clearly some structure is required.

Kubernetes provides that structure in two ways. First, it assigns an overall address space for the bridges on each node, and then assigns the bridges addresses within that space, based on the node the bridge is built on. Secondly it adds routing rules to the gateway at 10.100.0.1 telling it how packets destined for each bridge should be routed, i.e. which node’s eth0 the bridge can be reached through. Such a combination of virtual network interfaces, bridges, and routing rules is usually called an[overlay network](https://en.wikipedia.org/wiki/Overlay_network). When talking about kubernetes I usually call this network the “pod network” because it is an overlay network that allows pods to communicate back and forth on any node. Here is the drawing above with kubernetes in play:

![](https://cdn-images-1.medium.com/max/1600/1*oyGbXt7kStLd85ZT4it3oQ.png)

One thing that should jump out is that I’ve changed the name of the bridges from “docker0” to “cbr0.” Kubernetes does not use the standard docker bridge device and in fact “cbr” is short for “custom bridge.” I don’t know everything that is custom about it, but it is one of the important differences between docker running on kubernetes vs. a default installation. Another thing to note is that the address space assigned to the bridges in this example is 10.0.0.0/14. That’s taken from one of our staging clusters in Google Cloud and so is a real-world example. Your cluster may be assigned an entirely different range. Unfortunately there is no way to expose this using the`**kubectl**`utility at the moment, but on GCP you can run`**gcloud container****clusters describe <cluster>**`command and look for the`**clusterIpv4Cidr**`property.

Generally you won’t need to think about how this network functions. When a pod talks with another pod it most often does so through the abstraction of a service, a kind of software-defined proxy that will be the subject of the[next post in this series](https://medium.com/@betz.mark/understanding-kubernetes-networking-services-f0cb48e4cc82). But pod network addresses will pop up in logs and when debugging and in some scenarios you may need to explicitly route this network. For example traffic leaving a kubernetes pod bound for any address in the 10.0.0.0/8 range is not NAT’d by default, so if you communicate with services on another private network in that range you may need to set up rules to route the packets back to the pods. Hopefully this article will help you take the right steps to make such scenarios work correctly.


